{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "train = pd.read_csv('train_smooth_all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据量： 3853808\n",
      "验证集数据量： 153559\n"
     ]
    }
   ],
   "source": [
    "train['date'] = pd.to_datetime(train['date'])\n",
    "# 定义验证集的日期范围\n",
    "val_start_date1 = pd.to_datetime('2023-06-03')\n",
    "val_end_date1 = pd.to_datetime('2023-06-16')\n",
    "\n",
    "val_start_date2 = pd.to_datetime('2024-05-03')\n",
    "val_end_date2 = pd.to_datetime('2024-06-03')\n",
    "\n",
    "# 划分第一个验证集\n",
    "val1 = train[(train['date'] >= val_start_date1) & (train['date'] <= val_end_date1)]  # 2023-06-03 到 2023-06-16 的数据\n",
    "\n",
    "# 划分第二个验证集\n",
    "val2 = train[(train['date'] >= val_start_date2) & (train['date'] <= val_end_date2)]  # 2024-05-03 到 2024-06-03 的数据\n",
    "\n",
    "# 合并两个验证集\n",
    "val = pd.concat([val1, val2])\n",
    "\n",
    "# 划分训练集\n",
    "# 训练集是除了两个验证集之外的数据\n",
    "train_before_val1 = train[train['date'] < val_start_date1]  # 第一个验证集之前的数据\n",
    "train_between_vals = train[(train['date'] > val_end_date1) & (train['date'] < val_start_date2)]  # 两个验证集之间的数据\n",
    "train_after_val2 = train[train['date'] > val_end_date2]  # 第二个验证集之后的数据\n",
    "\n",
    "# 合并训练集\n",
    "train = pd.concat([train_before_val1, train_between_vals, train_after_val2])\n",
    "\n",
    "# 检查数据量\n",
    "print(\"训练集数据量：\", len(train))\n",
    "print(\"验证集数据量：\", len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          12.63\n",
       "2          34.55\n",
       "3          34.52\n",
       "4          35.92\n",
       "6           6.39\n",
       "           ...  \n",
       "4007362    26.56\n",
       "4007363    27.42\n",
       "4007364    33.39\n",
       "4007365    22.88\n",
       "4007366    32.10\n",
       "Name: sales, Length: 3853808, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_date = train['date']\n",
    "val_date = val['date']\n",
    "train.drop(['unique_id','date'], axis=1, inplace=True)\n",
    "val.drop(['unique_id','date'], axis=1, inplace=True)\n",
    "y_train = train['sales']\n",
    "y_val = val['sales']\n",
    "train = train.drop(columns=['sales'])\n",
    "val = val.drop(columns=['sales'])\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 定义随机森林回归模型\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    n_estimators=100,       # 树的数量\n",
    "    criterion='squared_error',  # 分裂标准\n",
    "    max_depth=None,         # 树的最大深度\n",
    "    min_samples_split=2,    # 分裂内部节点所需的最小样本数\n",
    "    min_samples_leaf=1,     # 叶节点所需的最小样本数\n",
    "    max_leaf_nodes=None,    # 最大叶节点数量\n",
    "    min_impurity_decrease = 0.0,  # 分裂节点的最小不纯度减少量\n",
    "    # max_features='auto',    # 寻找最佳分裂时考虑的特征数量\n",
    "    bootstrap=True,         # 是否使用 bootstrap 采样\n",
    "    random_state=42,         # 随机种子\n",
    "    n_jobs=-1,               # 使用所有可用的 CPU 核心\n",
    "    verbose=2                # 输出调优过程\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 268435456 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\program\\anaconda\\Lib\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\program\\anaconda\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\program\\anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\program\\anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 192, in _parallel_build_trees\n    tree._fit(\n  File \"d:\\program\\anaconda\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n  File \"sklearn\\\\tree\\\\_tree.pyx\", line 166, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn\\\\tree\\\\_tree.pyx\", line 285, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn\\\\tree\\\\_tree.pyx\", line 940, in sklearn.tree._tree.Tree._add_node\n  File \"sklearn\\\\tree\\\\_tree.pyx\", line 908, in sklearn.tree._tree.Tree._resize_c\n  File \"sklearn\\\\tree\\\\_utils.pyx\", line 35, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 268435456 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rf_regressor\u001b[38;5;241m.\u001b[39mfit(train, y_train)\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )(\n\u001b[0;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    495\u001b[0m         t,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    497\u001b[0m         X,\n\u001b[0;32m    498\u001b[0m         y,\n\u001b[0;32m    499\u001b[0m         sample_weight,\n\u001b[0;32m    500\u001b[0m         i,\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\program\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 268435456 bytes"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "rf_regressor.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练好的模型进行预测\n",
    "y_pred = rf_regressor.predict(val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# 计算均方误差 (MSE)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "# 计算 R² (决定系数)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "print(f'R² Score: {r2}')\n",
    "\n",
    "# 输出预测结果\n",
    "print('Predictions:', y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算分位数\n",
    "bins = np.quantile(y_val, [0.33, 0.66])\n",
    "\n",
    "# 将目标变量分为低、中、高三个区间\n",
    "y_val_binned = np.digitize(y_val, bins)\n",
    "\n",
    "# 输出区间范围\n",
    "print(f'Low range: y_val < {bins[0]:.2f}')\n",
    "print(f'Mid range: {bins[0]:.2f} <= y_val < {bins[1]:.2f}')\n",
    "print(f'High range: y_val >= {bins[1]:.2f}')\n",
    "\n",
    "# 计算每个区间的 MAE\n",
    "for i in range(3):\n",
    "    mask = y_val_binned == i\n",
    "    mae = mean_absolute_error(y_val[mask], y_pred[mask])\n",
    "    print(f'MAE for bin {i}: {mae:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 定义验证集的日期范围\n",
    "val_start_date1 = pd.to_datetime('2023-06-03')\n",
    "val_end_date1 = pd.to_datetime('2023-06-16')\n",
    "\n",
    "val_start_date2 = pd.to_datetime('2024-05-03')\n",
    "val_end_date2 = pd.to_datetime('2024-06-03')\n",
    "\n",
    "train['date'] = train_date\n",
    "val['date'] = val_date\n",
    "\n",
    "train['sales'] = y_train\n",
    "val['sales'] = y_val\n",
    "# 确保 train 和 val 的 date 列是 datetime 类型\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "val['date'] = pd.to_datetime(val['date'])\n",
    "\n",
    "# 将预测值添加到验证集中\n",
    "val['sales_hat'] = y_pred\n",
    "\n",
    "# 划分训练集为多个部分\n",
    "train_before_val1 = train[train['date'] < val_start_date1]  # 第一个验证集之前的数据\n",
    "train_between_vals = train[(train['date'] > val_end_date1) & (train['date'] < val_start_date2)]  # 两个验证集之间的数据\n",
    "train_after_val2 = train[train['date'] > val_end_date2]  # 第二个验证集之后的数据\n",
    "\n",
    "# 划分验证集为多个部分\n",
    "val1 = val[(val['date'] >= val_start_date1) & (val['date'] <= val_end_date1)]  # 第一个验证集\n",
    "val2 = val[(val['date'] >= val_start_date2) & (val['date'] <= val_end_date2)]  # 第二个验证集\n",
    "\n",
    "# 仅保留 2023 年和 2024 年的数据\n",
    "train_before_val1 = train_before_val1[train_before_val1['date'].dt.year.isin([2023, 2024])]\n",
    "train_between_vals = train_between_vals[train_between_vals['date'].dt.year.isin([2023, 2024])]\n",
    "train_after_val2 = train_after_val2[train_after_val2['date'].dt.year.isin([2023, 2024])]\n",
    "val1 = val1[val1['date'].dt.year.isin([2023, 2024])]\n",
    "val2 = val2[val2['date'].dt.year.isin([2023, 2024])]\n",
    "\n",
    "# 绘制图形\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# 绘制第一个验证集之前训练集的实际销售数据（蓝色）\n",
    "sns.lineplot(data=train_before_val1, x='date', y='sales', color='blue', label='Train Sales ')\n",
    "\n",
    "# 绘制两个验证集之间训练集的实际销售数据（蓝色）\n",
    "sns.lineplot(data=train_between_vals, x='date', y='sales', color='blue')\n",
    "\n",
    "# 绘制第二个验证集之后训练集的实际销售数据（蓝色）\n",
    "sns.lineplot(data=train_after_val2, x='date', y='sales', color='blue')\n",
    "\n",
    "# 绘制第一个验证集的实际销售数据（黄色）\n",
    "sns.lineplot(data=val1, x='date', y='sales', color='yellow', label='Validation True Sales')\n",
    "\n",
    "# 绘制第二个验证集的实际销售数据（黄色）\n",
    "sns.lineplot(data=val2, x='date', y='sales', color='yellow')\n",
    "\n",
    "# 绘制第一个验证集的预测销售数据（红色虚线）\n",
    "sns.lineplot(data=val1, x='date', y='sales_hat', color='red', linestyle='--', label='Predicted Sales')\n",
    "\n",
    "# 绘制第二个验证集的预测销售数据（红色虚线）\n",
    "sns.lineplot(data=val2, x='date', y='sales_hat', color='red', linestyle='--')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Sales and Sales Predictions Over Time (2023-2024)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制图形\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# 绘制第一个验证集的实际销售数据（黄色）\n",
    "sns.lineplot(data=val1, x='date', y='sales', color='yellow', label='Validation True Sales')\n",
    "\n",
    "# 绘制第一个验证集的预测销售数据（红色虚线）\n",
    "sns.lineplot(data=val1, x='date', y='sales_hat', color='red', linestyle='--', label='Predicted Sales')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Validation Set: True vs Predicted Sales (2023-2024)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制图形\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# 绘制第二个验证集的实际销售数据（黄色）\n",
    "sns.lineplot(data=val2, x='date', y='sales', color='yellow', label='Validation True Sales')\n",
    "\n",
    "# 绘制第二个验证集的预测销售数据（红色虚线）\n",
    "sns.lineplot(data=val2, x='date', y='sales_hat', color='red', linestyle='--', label='Predicted Sales')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Validation Set: True vs Predicted Sales (2023-2024)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
